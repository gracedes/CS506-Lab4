{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e534f5e3",
   "metadata": {},
   "source": [
    "# **Lab 4: Topic Modeling with Latent Semantic Analysis (LSA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df22bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.1.3)\n",
      "Requirement already satisfied: click in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kdesr\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading regex-2024.9.11-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "%pip install nltk scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0457d",
   "metadata": {},
   "source": [
    "\n",
    "### **Part 1: Loading and Preprocessing the BBC News Dataset**\n",
    "\n",
    "In this section, weâ€™ll load the dataset, preprocess the text by removing stopwords, tokenizing, and creating a term-document matrix using TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42990ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001.txt</td>\n",
       "      <td>Gallery unveils interactive tree</td>\n",
       "      <td>A Christmas tree that can receive text messag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002.txt</td>\n",
       "      <td>Jarre joins fairytale celebration</td>\n",
       "      <td>French musician Jean-Michel Jarre is to perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003.txt</td>\n",
       "      <td>Musical treatment for Capra film</td>\n",
       "      <td>The classic film It's A Wonderful Life is to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004.txt</td>\n",
       "      <td>Richard and Judy choose top books</td>\n",
       "      <td>The 10 authors shortlisted for a Richard and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005.txt</td>\n",
       "      <td>Poppins musical gets flying start</td>\n",
       "      <td>The stage adaptation of children's film Mary ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename                              title  \\\n",
       "0  001.txt   Gallery unveils interactive tree   \n",
       "1  002.txt  Jarre joins fairytale celebration   \n",
       "2  003.txt   Musical treatment for Capra film   \n",
       "3  004.txt  Richard and Judy choose top books   \n",
       "4  005.txt  Poppins musical gets flying start   \n",
       "\n",
       "                                             content  \n",
       "0   A Christmas tree that can receive text messag...  \n",
       "1   French musician Jean-Michel Jarre is to perfo...  \n",
       "2   The classic film It's A Wonderful Life is to ...  \n",
       "3   The 10 authors shortlisted for a Richard and ...  \n",
       "4   The stage adaptation of children's film Mary ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Load the BBC news dataset\n",
    "data = pd.read_csv ('bbc_news_data.csv')\n",
    "\n",
    "# Check the first few rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2ea5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kdesr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-Document Matrix Shape: (1204, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# A simple function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    return ' '.join([word.lower() for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# TODO: Apply the preprocessing function to the 'content' column\n",
    "data['processed_content'] = [preprocess_text(t) for t in data['content']]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.05, max_features=1000, ngram_range=(1, 2))\n",
    "\n",
    "# TODO: Create the term-document matrix\n",
    "term_doc_matrix = vectorizer.fit_transform(data['processed_content'])\n",
    "\n",
    "# Get the terms (feature names) from the vectorizer\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the shape of the term-document matrix\n",
    "print(f\"Term-Document Matrix Shape: {term_doc_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6e807",
   "metadata": {},
   "source": [
    "\n",
    "### **Part 2: Applying SVD to the Term-Document Matrix**\n",
    "\n",
    "In this section, we will apply **Singular Value Decomposition (SVD)** to reduce the term-document matrix into its latent structure and identify the topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5345c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# TODO: Define the number of components\n",
    "num_components = ...\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=num_components)\n",
    "\n",
    "# TODO: Fit the SVD model\n",
    "svd_matrix = svd_model.fit_transform(...)\n",
    "\n",
    "# Show the resulting latent space (topic space)\n",
    "print(f\"Latent Topic Matrix Shape: {svd_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a39897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get the top terms for each topic\n",
    "\n",
    "num_top_words = ... # TODO: Adjust this until you can easily identify the topics\n",
    "\n",
    "for i, topic in enumerate(svd_model.components_):\n",
    "    top_term_indices = ... # TODO: Get the indices of the top terms\n",
    "    top_terms = [terms[i] for i in top_term_indices]\n",
    "    print(f\"Topic {i+1}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b641c7",
   "metadata": {},
   "source": [
    "\n",
    "### **Part 3: Labeling the Topics**\n",
    "\n",
    "TODO: Using the terms extracted from each topic, try to assign labels that best describe what each topic is about.\n",
    "\n",
    "- **Topic 1**: ...\n",
    "- **Topic 2**: ...\n",
    "- **Topic 3**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284765ec",
   "metadata": {},
   "source": [
    "\n",
    "### **Summary & Takeaways**\n",
    "\n",
    "In this lab, you have:\n",
    "1. Preprocessed the BBC News dataset and created a term-document matrix using TF-IDF.\n",
    "2. Applied SVD to reduce the term-document matrix into a lower-dimensional space, revealing hidden topics.\n",
    "3. Examined the most significant terms in each topic and interpreted their meaning.\n",
    "4. Labeled the topics based on the terms and document clusters.\n",
    "\n",
    "You now have a better understanding of how **LSA** can reveal hidden topics in a collection of text documents and how similar documents can group together based on their content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
